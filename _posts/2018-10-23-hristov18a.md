---
title: Interpretable Latent Spaces for Learning from Demonstration
abstract: 'Effective human-robot interaction, such as in robot learning from human
  demonstration, requires the learning agent to be able to ground abstract concepts
  (such as those contained within instructions) in a corresponding high-dimensional
  sensory input stream from the world. Models such as deep neural networks, with high
  capacity through their large parameter spaces, can be used to compress the high-dimensional
  sensory data to lower dimensional representations. These low-dimensional representations
  facilitate symbol grounding, but may not guarantee that the representation would
  be human-interpretable. We propose a method which utilises the grouping of user-defined
  symbols and their corresponding sensory observations in order to align the learnt
  compressed latent representation with the semantic notions contained in the abstract
  labels. We demonstrate this through experiments with both simulated and real-world
  object data, showing that such alignment can be achieved in a process of physical
  symbol grounding. '
keywords: " disentanglement learning, model interpretability, symbol grounding"
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hristov18a
month: 0
tex_title: Interpretable Latent Spaces for Learning from Demonstration
firstpage: 957
lastpage: 968
page: 957-968
order: 957
cycles: false
bibtex_author: Hristov, Yordan and Lascarides, Alex and Ramamoorthy, Subramanian
author:
- given: Yordan
  family: Hristov
- given: Alex
  family: Lascarides
- given: Subramanian
  family: Ramamoorthy
date: 2018-10-23
address: 
publisher: PMLR
container-title: Proceedings of The 2nd Conference on Robot Learning
volume: '87'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 10
  - 23
pdf: http://proceedings.mlr.press/v87/hristov18a/hristov18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
