---
title: Bayesian RL for Goal-Only Rewards
abstract: 'We address the challenging problem of reinforcement learning under goal-only
  rewards [1], where rewards are only non-zero when the goal is achieved. This reward
  definition alleviates the need for cumbersome reward engineering, making the reward
  formulation trivial. Classic exploration heuristics such as Boltzmann or epsilon-greedy
  exploration are highly inefficient in domains with goal-only rewards. We solve this
  problem by leveraging value function posterior variance information to direct exploration
  where uncertainty is higher. The proposed algorithm (EMU-Q) achieves data-efficient
  exploration, and balances exploration and exploitation explicitly at a policy level
  granting users more control over the learning process. We introduce general features
  approximating kernels, allowing to greatly reduce the algorithm complexity from
  O(N^3) in the number of transitions to O(M^2) in the number of features. We demonstrate
  EMU-Q is competitive with other exploration techniques on a variety of continuous
  control tasks and on a robotic manipulator. '
keywords: " Reinforcement learning, Exploration, Continuous control"
layout: inproceedings
series: Proceedings of Machine Learning Research
id: morere18a
month: 0
tex_title: Bayesian RL for Goal-Only Rewards
firstpage: 386
lastpage: 398
page: 386-398
order: 386
cycles: false
bibtex_author: Morere, Philippe and Ramos, Fabio
author:
- given: Philippe
  family: Morere
- given: Fabio
  family: Ramos
date: 2018-10-23
address: 
publisher: PMLR
container-title: Proceedings of The 2nd Conference on Robot Learning
volume: '87'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 10
  - 23
pdf: http://proceedings.mlr.press/v87/morere18a/morere18a.pdf
extras:
- label: Source code
  link: https://github.com/PhilippeMorere/EMU-Q
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
